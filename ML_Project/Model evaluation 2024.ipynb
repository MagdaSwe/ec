{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f52787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import canny\n",
    "\n",
    "from scipy.ndimage import find_objects, label\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import dump\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7b8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"].astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f149b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2842e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42) \n",
    "X_train_normalized = X_train / 255.0\n",
    "X_test_normalized = X_test / 255.0\n",
    "X_val_normalized = X_val / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19206288",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "plt.bar(unique, counts)\n",
    "plt.title(\"Frequency of Each Digit in the Dataset\")\n",
    "plt.xlabel(\"Digit\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(unique)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a859099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9733b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize pixel intensity distribution of a sample\n",
    "plt.hist(X_train_normalized[0], bins=20, range=[0,1])\n",
    "plt.title(\"Pixel Intensity Distribution for a Sample Digit\")\n",
    "plt.xlabel(\"Pixel Intensity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Example: Visualizing multiple samples\n",
    "fig, axes = plt.subplots(10, 10, figsize=(9, 9),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X[i].reshape(28, 28), cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(y[i]),\n",
    "            transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7816fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique_digits = np.unique(y_train)\n",
    "\n",
    "for digit in unique_digits:\n",
    "    digit_indices = np.where(y_train == digit)[0]\n",
    "    digit_images = X_train[digit_indices]\n",
    "    \n",
    "    # Assuming the images are flattened. If not, you might need to adjust.\n",
    "    max_intensity = np.max(digit_images)\n",
    "    min_intensity = np.min(digit_images)\n",
    "    avg_intensity = np.mean(digit_images)\n",
    "    \n",
    "    print(f\"Digit: {digit}, Max intensity: {max_intensity}, Min intensity: {min_intensity}\",f\"Digit: {digit}, Average intensity: {avg_intensity}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique_digits = np.unique(y_train)\n",
    "avg_intensity = []\n",
    "\n",
    "for digit in unique_digits:\n",
    "    digit_indices = np.where(y_train == digit)[0]\n",
    "    digit_images = X_train[digit_indices]\n",
    "    \n",
    "    # Calculate the average intensity for each digit\n",
    "    avg_intensity.append(np.mean(digit_images))\n",
    "\n",
    "# Plotting\n",
    "plt.bar(unique_digits, avg_intensity, color='skyblue')\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Average Pixel Intensity')\n",
    "plt.title('Average Pixel Intensity by Digit')\n",
    "plt.xticks(unique_digits)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn_clf = KNeighborsClassifier(n_jobs= -1)\n",
    "knn_clf.fit(X_train_normalized, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validation set for hyperparameter tuning or model selection\n",
    "val_accuracy = knn_clf.score(X_val_normalized, y_val)\n",
    "print(\"Validation set accuracy:\", val_accuracy)\n",
    "\n",
    "# After finalizing the model and its hyperparameters, evaluate on the test set\n",
    "test_accuracy = knn_clf.score(X_test_normalized, y_test)\n",
    "print(\"Test set accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff860170",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c293521",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(knn_clf, 'model_rf_clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3360342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03910d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96054dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "svm_clf = SVC(gamma='auto', kernel='rbf')\n",
    "svm_clf.fit(X_train_normalized, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validation set for hyperparameter tuning or model selection\n",
    "val_accuracy_svm = svm_clf.score(X_val_normalized, y_val)\n",
    "print(\"Validation set accuracy:\", val_accuracy_svm)\n",
    "\n",
    "# After finalizing the model and its hyperparameters, evaluate on the test set\n",
    "test_accuracy_svm = svm_clf.score(X_test_normalized, y_test)\n",
    "print(\"Test set accuracy:\", test_accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(svm_clf, 'svm_clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "rf_clf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac711f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validation set for hyperparameter tuning or model selection\n",
    "val_accuracy_clf = rf_clf.score(X_val_normalized, y_val)\n",
    "print(\"Validation set accuracy:\", val_accuracy_clf)\n",
    "\n",
    "# After finalizing the model and its hyperparameters, evaluate on the test set\n",
    "test_accuracy_clf = rf_clf.score(X_test_normalized, y_test)\n",
    "print(\"Test set accuracy:\", test_accuracy_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dump(rf_clf, 'model_rf_clf.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a89493",
   "metadata": {},
   "source": [
    "Gör allt på en gång"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models and their parameter grids\n",
    "models_and_parameters = [\n",
    "    (SVC(), {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }),\n",
    "    (RandomForestClassifier(), {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }),\n",
    "    (KNeighborsClassifier(), {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    })\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to hold the best models\n",
    "best_models = {}\n",
    "\n",
    "# Loop over each model and its hyperparameters\n",
    "for model, param_grid in models_and_parameters:\n",
    "    print(f\"Starting GridSearchCV for {model.__class__.__name__}...\")\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train_normalized, y_train)\n",
    "    \n",
    "    # Save the best model\n",
    "    best_models[model.__class__.__name__] = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy for {model.__class__.__name__}: {grid_search.best_score_}\\n\")\n",
    "\n",
    "# Assuming the models were SVC, RandomForestClassifier, and KNeighborsClassifier,\n",
    "# and they are now saved in best_models\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svc', best_models['SVC']),\n",
    "        ('rf', best_models['RandomForestClassifier']),\n",
    "        ('knn', best_models['KNeighborsClassifier'])\n",
    "    ],\n",
    "    voting='soft'  # Change to 'soft' if all models support predict_proba\n",
    ")\n",
    "\n",
    "# Fit the voting classifier on the training data\n",
    "voting_clf.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Evaluate the voting classifier on the test data\n",
    "test_accuracy = voting_clf.score(X_test_normalized, y_test)\n",
    "print(\"Test set accuracy with Voting Classifier:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_models.keys())\n",
    "\n",
    "# Explicitly mentioning the directory if necessary, for example, the current directory\n",
    "dump(best_models['RandomForestClassifier'], 'rf_best_model.joblib')\n",
    "dump(best_models['SVC'], 'svc_best_model.joblib')\n",
    "\n",
    "dump(best_models['KNeighborsClassifier'], 'knn_best_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bca4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "# Load the models\n",
    "svc_best_model = load('svc_best_model.joblib')\n",
    "rf_best_model = load('rf_best_model.joblib')\n",
    "knn_best_model = load('knn_best_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a30ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for filename in os.listdir('.'):\n",
    "    if filename.endswith('.joblib'):\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_test_normalized and y_test are available and properly preprocessed\n",
    "\n",
    "# Make predictions with SVC\n",
    "svc_predictions = svc_best_model.predict(X_test_normalized)\n",
    "# Make predictions with RandomForest\n",
    "rf_predictions = rf_best_model.predict(X_test_normalized)\n",
    "# Make predictions with KNeighborsClassifier\n",
    "knn_predictions = knn_best_model.predict(X_test_normalized)\n",
    "# Evaluate SVC model\n",
    "print(\"SVC Model Evaluation\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, svc_predictions):.4f}\")\n",
    "print(classification_report(y_test, svc_predictions))\n",
    "\n",
    "# Evaluate RandomForest model\n",
    "print(\"RandomForest Model Evaluation\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, rf_predictions):.4f}\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "\n",
    "# Evaluate KNeighborsClassifier model\n",
    "print(\"KNeighborsClassifier Model Evaluation\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, knn_predictions):.4f}\")\n",
    "print(classification_report(y_test, knn_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ff80a",
   "metadata": {},
   "source": [
    "SVC vann.. går idare och tränar med mer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "# Instantiate SVC with the best parameters\n",
    "svc_clf = SVC(C=10, gamma='scale', kernel='rbf', probability=True)\n",
    "\n",
    "# Wrap it in a BaggingClassifier\n",
    "bagging_clf = BaggingClassifier(svc_clf, n_estimators=10, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "# Train the ensemble classifier on your training data\n",
    "bagging_clf.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Evaluate the ensemble classifier on your test data\n",
    "test_accuracy = bagging_clf.score(X_test_normalized, y_test)\n",
    "print(\"Bagging SVC Test set accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = SVC(C=10, gamma='scale', kernel='rbf', probability=True)\n",
    "svc_clf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create 40 separable points\n",
    "X, y = make_blobs(n_samples=200, centers=2, random_state=6)\n",
    "\n",
    "# Fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(min(X[:, 0]), max(X[:, 0]), 30)\n",
    "yy = np.linspace(min(X[:, 1]), max(X[:, 1]), 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot data points and color the areas of different classes\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n",
    "plt.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "            linestyles=['--', '-', '--'])\n",
    "# Plot support vectors\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "            linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title('SVM Decision Boundary with Support Vectors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c40577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class information\n",
    "print(\"Class labels:\", np.unique(y))\n",
    "print(\"Samples per class:\", [np.sum(y == i) for i in np.unique(y)])\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(min(X[:, 0]), max(X[:, 0]), 30)\n",
    "yy = np.linspace(min(X[:, 1]), max(X[:, 1]), 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot data points and color the areas of different classes\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n",
    "plt.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "# Plot support vectors\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "            linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title('SVM Decision Boundary with Support Vectors')\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f9f1f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20caac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test set\n",
    "predictions_bagging = bagging_clf.predict(X_test_normalized)\n",
    "probabilities = bagging_clf.predict_proba(X_test_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SVC model\n",
    "print(\"SVC Model Evaluation\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions_bagging):.4f}\")\n",
    "print(classification_report(y_test, predictions_bagging))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ea6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, predictions_bagging)\n",
    "print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
    "\n",
    "# Generating a classification report\n",
    "report = classification_report(y_test, predictions_bagging)\n",
    "print(\"Classification report:\\n\", report)\n",
    "\n",
    "# Generating a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predictions_bagging)\n",
    "print(\"Confusion matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468aef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "# Binarize the output\n",
    "y_test_binarized = label_binarize(y_test, classes=np.unique(y_test))\n",
    "\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], probabilities[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multiclass')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea8dda",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Plot all ROC curves\n",
    "Train Your Bagging Classifier with the best-found parameters and ensure it's capable of outputting probabilities (which it should be since you're using SVC with probability=True as the base estimator).\n",
    "\n",
    "Make Probability Predictions on your test set using the predict_proba method of your trained bagging classifier. This gives you the probability estimates required for ROC curve analysis.\n",
    "\n",
    "Prepare Your Test Labels for ROC analysis by binarizing them. This step is crucial for multiclass ROC curve plotting, where you're treating each class as a binary classification problem (the class vs. all others).\n",
    "\n",
    "Compute and Plot ROC Curves for each class using the probability estimates. This gives you insight into how well your model can distinguish between each class and the rest.\n",
    "\n",
    "Calculate and Plot the Macro-average ROC Curve to get a single summary metric of your model's overall performance across all classes. This macro-average curve helps in understanding the model's general ability to classify instances correctly across a balanced set of classes.\n",
    "\n",
    "The probability predictions (probabilities = bagging_clf.predict_proba(X_test_normalized)) generated in step 2 are indeed based on your bagging model predictions. This entire process allows you to evaluate how well your ensemble model, specifically the bagging classifier using an SVC base estimator, performs in terms of distinguishing between the different classes in a multiclass classification setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at these points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot the macro-average ROC curve\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Macro-average ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199cbdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_misclassified_images(indices, X_data, y_true, y_pred, cols=5, rows=5):\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in zip(indices[:cols*rows], axes):\n",
    "        image = X_data[i].reshape(28, 28)  # Adjust depending on your data's shape\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.set_title(f'True: {y_true[i]}\\nPred: {y_pred[i]}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Now call the function using the misclassified indices\n",
    "plot_misclassified_images(misclassified_indices, X_test, y_test, predictions_bagging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70199de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be031a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indices = np.where((svc_predictions != y_test) & ((y_test == 9) | (y_test == 7)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25811fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d347c20",
   "metadata": {},
   "source": [
    "Look for Patterns\n",
    "As you review the misclassified images, consider questions like:\n",
    "\n",
    "Are the misclassified 9s written in a particular style that might resemble 7s (or vice versa)?\n",
    "Is there a consistent reason why these digits are confused, such as similar strokes, angles, or intersections in their shapes?\n",
    "Are there preprocessing steps that might have distorted these digits or made them less recognizable?\n",
    "4. Addressing Common Issues\n",
    "Depending on what you find, you might consider:\n",
    "\n",
    "Enhancing Data Preprocessing: Adjust your preprocessing steps to better preserve the distinguishing features of 9s and 7s.\n",
    "Data Augmentation: Introduce variations in your training data that mimic the issues you've identified in the misclassified instances.\n",
    "Feature Engineering: If using traditional machine learning methods, consider engineering features that better capture the distinct aspects of 9s and 7s.\n",
    "Model Adjustments: Experiment with different architectures or models that might be better at capturing the nuances between these digits.\n",
    "5. Iterate\n",
    "Error analysis is an iterative process. After making adjustments based on your findings, retrain your model and see if the performance on 9s and 7s improves. Repeat the error analysis as necessary to continue refining your model's ability to correctly classify these challenging instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b817f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming `X_test_normalized` and `y_test` are your test data and labels\n",
    "predictions = svc_best_model.predict(X_test_normalized)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions):.4f}\")\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your model can provide probability estimates\n",
    "# For binary classification or one-vs-rest in multiclass\n",
    "probabilities = svc_best_model.predict_proba(X_test_normalized)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test, probabilities, pos_label=positive_class_label)  # Define positive_class_label appropriately\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04813fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ffdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9f9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, predictions):.4f}\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    # Predict on the validation data\n",
    "    conf_matrix = confusion_matrix(y_test ,predictions)\n",
    "    ConfusionMatrixDisplay(conf_matrix).plot()\n",
    "    #dump(model, 'model_',model,'_.joblib')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def evaluate_model_with_cv(model, X_train, y_train, cv=5):\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv)\n",
    "    \n",
    "    # Print cross-validation metrics\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "    print(f\"Average CV Accuracy: {np.mean(cv_scores):.4f}\")\n",
    "    print(f\"Standard Deviation in CV: {np.std(cv_scores):.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7602bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "   \n",
    "def plot_learning_curve(model, X, y):\n",
    "    train_sizes=np.linspace(.1, 1.0, 3)  # Using fewer points\n",
    "    \n",
    "    # Start with a subset if needed (e.g., X[:5000], y[:5000]) for large datasets\n",
    "    # Adjust n_jobs to parallelize computation if your setup supports it\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X, y, train_sizes=train_sizes, cv=5, n_jobs=-1)\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "    \n",
    "    plt.plot(train_sizes, train_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with a specific model and your data\n",
    "# plot_learning_curve(model, X_train_normalized, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9c0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0ac4341",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdef45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d73f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745b630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ab721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbd878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bec83b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25267199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21dd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392a8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19750b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9c0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7ae4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1f809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6756bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3bd0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417eb120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bec45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85a2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
